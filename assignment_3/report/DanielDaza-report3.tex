\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{booktabs}

\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\nl}{\newline}

\title{Deep Learning\\Assignment 3: Deep Generative Models}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Daniel Daza\\
  University of Amsterdam\\
  \texttt{daniel.dazacruz@student.uva.nl} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
-
\end{abstract}


\section{Variational Autoencoders}

\subsection*{Question 1.1}

\begin{enumerate}
\item Both the AE and VAE can be used to find a code underlying an observation. Ideally, this code should serve as a compressed representation of the input that contains the right amount of information about it, finding a balance that ignores irrelevant details, and keeps meaningful features. In this sense, the models are not different. However, the VAE achieves this by solving a problem of statistical inference, thus enabling the code to represent uncertainty. For some applications, this can be part of the main function, making it different from the AE. An example of this use is the Bayesian Skipgram \cite{bravzinskas2017embedding}, where words are embedded as probability distributions that capture uncertainty in the representation. This allows to represent ambiguous words with distributions of high variance, and viceversa; and to compute similarities between words via the KL divergence. This would not be possible with the AE, where the code is deterministic and does not have the ability to represent uncertainty.

\item The AE is not generative, because it does not model the data-generating distribution, either explicitly or implicitly. Instead, it finds a deterministic map $\mathbf{z} = f(\mathbf{x})$ for the encoder, and $\mathbf{x} = g(\mathbf{z})$ for the decoder. These functions do not have a probabilistic interpretation, therefore they do not allow for the calculation of likelihoods or expectations.

\item Both the AE and the VAE are trained in an unsupervised way, and the training procedure is based on the reconstruction of the inputs. For this reason, a VAE can be used in place of a standard AE, provided that the additional term for the KL divergence between the approximate posterior and the prior is added to the loss function. For downstream tasks, such as classification, the mean parameterized by the encoder of the VAE can be used in place of the deterministic code provided by the AE.

\item The aspect that enables the VAE to be generative is the ability to define the probability distribution of the latent variable $p(\mathbf{z})$, and the distribution $p(\mathbf{x}\vert\mathbf{z})$ of the observation conditioned on the latent variable. With these components, the VAE learns the joint distribution $p(\mathbf{x},\mathbf{z})$, making it a generative model. A standard AE is only concerned with finding a map to copy the input to the output, and in general it does not have any incentive to find a code that generalizes to unseen observations.
\end{enumerate}

\subsection*{Question 1.2}

According to the graphical model described by the VAE, to obtain a sample of $\mathbf{x}$ we can use ancestral sampling, that is, first we sample the root of the directed graphical model, which in this case corresponds to the latent variable $\mathbf{z}$, and then we use this sample to obtain a sample of its children, corresponding to $\mathbf{x}$. Since we are assuming the pixels to be independent conditioned on $\mathbf{z}$, we only need $\mathbf{z}^{(m)}$ (the $m$-th element of $\mathbf{z}$) to sample $\mathbf{x}^{(m)}$. The sample procedure is then the following:

\begin{align*}
\mathbf{z} &\sim \mathcal{N}(0, \mathbf{I}_D)\\
\mathbf{x}^{(m)} &\sim \text{Bern}(f_\theta(\mathbf{z})^{(m)})\ \forall n = 1, \dots, M
\end{align*}

\subsection*{Question 1.3}

The standard normal prior of the VAE is not a restrictive assumption because samples from it are transformed by the decoding network, $f_\theta(\cdot)$, which can be given arbitrary capacity. We can specify a decoder with a large number of parameters and multiple layers so that it represents a nonlinear transformation representing a map from the simple standard prior, to a sufficiently complicated distribution.

\subsection*{Question 1.4}

\begin{description}
\item[(a)] We can approximate the expectation with a Monte Carlo estimate:
\begin{align*}
\log\int p(\mathbf{x}_n\vert\mathbf{z}_n)p(\mathbf{z}_n)d\mathbf{z}_n &= \log\mathbb{E}_{p(\mathbf{z}_n)}[p(\mathbf{x}_n\vert\mathbf{z}_n)] \\
&\approx \frac{1}{L}\sum_{i=1}^{L} p(\mathbf{x}_n\vert\mathbf{z}_i)
\end{align*}
with $\mathbf{z}_i\sim\mathcal{N}(0, \mathbf{I}_D)$.

\item[(b)] Intuitively, the previous procedure estimates $\log p(\mathbf{x}_n)$ by obtaining a sample of $\mathbf{z}_n$ from the prior, and using it to calculate the probability that the model assigns to $\mathbf{x}_n$. We expect to obtain a good estimate by averaging this probability over multiple sampling steps. The problem with this approach is that values of $\mathbf{z}_n$ where $p(\mathbf{x}_n\vert\mathbf{z}_n)$ assigns a high probability to an observation can lie in a region much smaller than the broad regions that the prior covers. This means that most of the time we will be obtaining samples of $\mathbf{z}_n$ that assign low probability to $\mathbf{x}_n$. Hence, to obtain a good estimate, we will need many samples. This effects worsens as the dimension of $\mathbf{z}$ increases, making the approach very inefficient.
\end{description}

\subsection*{Question 1.5}

\begin{description}
\item[(a)] The smallest KL divergence is obtained when the two distributions are the same. Therefore, if $\mu_q = 0$ and $\sigma_q=1$, $D_{\text{KL}}(q\Vert p) = 0$.
From the definition of the KL divergence, we can see that if for some $x$, $p(x)\rightarrow 0$ and $q(x)>>0$, then $q(x)/p(x)\rightarrow\infty$. We can then make $D_{\text{KL}}(q\Vert p)$ large by letting $\mu\rightarrow\infty$ and $\sigma_q = \mu_q$, so that $q(x)$ assigns high probability to regions where $p(x)$ assigns low probability.

\item[(b)] The KL divergence between a standard prior and a multivariate Gaussian can be found in the original publication \cite{kingma2014vae}. In the univariate case we obtain:
\begin{equation*}
D_{\text{KL}}(q\Vert p) = \frac{1}{2}(\sigma_q^2 + \mu_q^2 - \log(\sigma_q^2) - 1)
\end{equation*}
\end{description}

\subsection*{Question 1.6}

Since the KL divergence is nonnegative, we can write

\begin{align*}
\log p(\mathbf{x}_n) &= \mathbb{E}[\log p(\mathbf{x}_n\vert Z)] - D_{\text{KL}}(q(Z\vert\mathbf{x}_n)\Vert p(Z)) + D_{\text{KL}}(q(Z\vert\mathbf{x}_n)\Vert p(Z\vert\mathbf{x}_n)) \\
&\geq \mathbb{E}[\log p(\mathbf{x}_n\vert Z)] - D_{\text{KL}}(q(Z\vert\mathbf{x}_n)\Vert p(Z))
\end{align*}

Therefore this last expression is a lower bound for the log-probability.

\subsection*{Question 1.7}

If we wanted to optimize the log-probability, we would need to calculate the true posterior distribution $p(\mathbf{z}\vert\mathbf{x})$:
\begin{align*}
p(\mathbf{z}\vert\mathbf{x}) &= \frac{p(\mathbf{z})p(\mathbf{x}\vert\mathbf{z})}{p(\mathbf{x})} \\
&= \frac{p(\mathbf{z})p(\mathbf{x}\vert\mathbf{z})}{\int p(\mathbf{z})p(\mathbf{x}\vert\mathbf{z})d\mathbf{z}}
\end{align*}

The integral in the denominator is not tractable, so we don't have direct access to the posterior.

\subsection*{Question 1.8}
When the lower bound is maximized, this causes the log-probability to increase, and the KL divergence between the approximate and true posteriors to decrease. This means that the model assigns higher probability to the observations, while improving the approximation $q(\mathbf{z}\vert\mathbf{x})$ to be closer to the true posterior $p(\mathbf{z}\vert\mathbf{x})$. This circumvents the problem of an intractable posterior because if the approximate and true posteriors match, maximizing the lower bound will directly maximize the log-probability.

\subsection*{Question 1.9}

If we only use one sample of $\mathbf{z}$, then we have the following loss for a data point: 
\begin{equation*}
-\log p_\theta(\mathbf{x}\vert Z) + D_{\text{KL}}(q_\phi(Z\vert\mathbf{x})\Vert p(Z))
\end{equation*}

To minimize the first term, the model must assign a high probability to the observation $\mathbf{z}$ given the latent variable. If the decoder can use a sample of the latent variable and produce a reconstruction close to the observation, this will mean that the observation is likely under the model. For this reason this loss is also known as the reconstruction loss.

To minimize the second term, the KL divergence between the prior and the approximate posterior must be low. This enforces a constraint in the parameters of the encoder, such that they are free to change in order to create a useful mapping from the observation to the latent variable, but not so much as to deviate from a predefined structure determined by the prior distribution. This term thus acts as a regularizer of the parameters of the model.

\subsection*{Question 1.10}

In the following we will omit the subscript in $\mathbf{x}_n$ as, we will specify the loss for a single sample.

The first step to calculate the loss is to obtain samples of the latent variable from the encoder, to estimate the reconstruction loss. This step comprises a forward pass through the decoder with the observation $\mathbf{x}$ as an input, and a step where $L$ samples of $\mathbf{z}$ are drawn:
\begin{align*}
\boldsymbol{\mu}_e &= \mu_\phi(\mathbf{x}) \\
\boldsymbol{\sigma}_e &= \Sigma_\phi(\mathbf{x}) \\
\mathbf{z}_i &\sim \mathcal{N}(\mathbf{z} \vert \boldsymbol{\mu}_e, \text{diag}(\boldsymbol{\sigma}_e)),\ i = 1, \dots, L
\end{align*}

The samples are then passed through the decoder to obtain the parameters of the Bernoulli distribution:
\begin{equation*}
\boldsymbol{\mu}_{di} = f_\theta(\mathbf{z}_i),\ i = 1, \dots, L
\end{equation*}

We can now estimate the reconstruction loss as follows:
\begin{align*}
\mathcal{L}_n^{\text{recon}} &= -\mathbb{E}_{q_\phi(Z\vert\mathbf{x})}[\log p_\theta(\mathbf{x}\vert Z)] \\
&\approx -\frac{1}{L}\sum_{i=1}^L \log p_\theta(\mathbf{x}\vert \mathbf{z}_i) \\
&=
-\frac{1}{L}\sum_{i=1}^L\log \prod_{m=1}^M \text{Bern}(\mathbf{x}^{(m)}\vert \boldsymbol{\mu}_{di}^{(m)}) \\
&=
-\frac{1}{L}\sum_{i=1}^L\sum_{m=1}^M \mathbf{x}^{(m)}\log\boldsymbol{\mu}_{di}^{(m)} + (1-\mathbf{x}^{(m)})\log(1 -\boldsymbol{\mu}_{di}^{(m)})
\end{align*}

To calculate the regularization term, we can make use of the closed form expression of the KL divergence. Previously we specified it assuming univariate Gaussians, and in the multivariate case we obtain:

\begin{align*}
\mathcal{L}^{\text{reg}} &= \mathcal{} D_\text{KL}(q_\phi(Z\vert\mathbf{x})\Vert p(Z)) \\
&=
\frac{1}{2}\sum_{i=1}^D ((\boldsymbol{\sigma}_e^2)^{(i)} + (\boldsymbol{\mu}_e^2)^{(i)} - \log(\boldsymbol{\sigma}_e^2)^{(i)} - 1)
\end{align*}

\subsection*{Question 1.11}

The gradient of the loss with respect to the variational parameters $\phi$ is required because it contains the information required to update the parameters of the encoder. The encoder influences the loss through the reconstruction, as samples from the approximate posterior are required to obtain the reconstructions. It also affects the loss through the regularization term. Therefore, if we want to optimize the VAE objective, we need to compute $\nabla\mathcal{L}_\phi$.

The problem with obtaining this gradient is that the steps to calculate the loss involve sampling from a probability distribution parameterized by the encoder, which is an operation that is not differentiable with respect to the parameters of the encoder. This can be solved with the reparameterization trick, which consists of obtaining a sample from a distribution that is not parameterized by the encoder, and passing it through a deterministic function such that the result is equivalent to sampling from the approximate posterior. This function is chosen so that it is differentiable with respect to $\phi$, so we can calculate the required gradients.

\subsection*{Question 1.12}







































\bibliographystyle{unsrt}
\bibliography{refs}
\end{document}
